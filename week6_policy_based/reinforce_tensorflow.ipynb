{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow (3 pts)Â¶\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable.\n",
    "\n",
    "Authors: [Practical_RL](https://github.com/yandexdataschool/Practical_RL) course team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0cf12f0390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqVMGvCR1yoCXpE4Z8JLUqf8PwS6MZvzJm1kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "#gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env,'env'):\n",
    "    env=env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheshank/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#create input variables. We only need <s,a,R> for REINFORCE\n",
    "states = tf.placeholder('float32',(None,)+state_dim,name=\"states\")\n",
    "actions = tf.placeholder('int32',name=\"action_ids\")\n",
    "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sheshank/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# <define network graph using raw tf or any deep learning library>\n",
    "\n",
    "layer1 = tf.contrib.layers.fully_connected(states, 100, activation_fn = tf.nn.relu)\n",
    "layer2 = tf.contrib.layers.fully_connected(layer1, 50, activation_fn = tf.nn.relu)\n",
    "logits = tf.layers.dense(layer2, n_actions) #<linear outputs (symbolic) of your network>\n",
    "\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function to pick action in one given state\n",
    "get_action_proba = lambda s: policy.eval({states:[s]})[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get probabilities for parti\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# policy objective as in the last formula. please use mean, not sum.\n",
    "# note: you need to use log_policy_for_actions to get log probabilities for actions taken\n",
    "\n",
    "J = tf.reduce_mean(log_policy_for_actions * cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#regularize with entropy\n",
    "entropy = - tf.reduce_mean(policy * log_policy) #<compute entropy. Don't forget the sign!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = [v for v in tf.trainable_variables()] #<a list of all trainable weights in your network>\n",
    "\n",
    "#weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
    "loss = -J - (0.1 * entropy)\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss,var_list=all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, #rewards at each step\n",
    "                           gamma = 0.99 #discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    #<your code here>\n",
    "    cumulative_rewards = np.empty_like(rewards)\n",
    "    cumulative_rewards = cumulative_rewards.astype(float)\n",
    "    cumulative_rewards[-1] = rewards[-1]\n",
    "    \n",
    "    for index in range(len(rewards)-2, -1, -1):\n",
    "        discount = cumulative_rewards[index+1]*gamma\n",
    "        reward = rewards[index]\n",
    "        cumulative_rewards[index] = discount + reward\n",
    "        \n",
    "    return cumulative_rewards #<array of cumulative rewards>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(_states,_actions,_rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states:_states,actions:_actions,cumulative_rewards:_cumulative_rewards})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #action probabilities array aka pi(a|s)\n",
    "        action_probas = get_action_proba(s)\n",
    "        \n",
    "        a = np.random.choice(range(n_actions),p=action_probas)#<pick random action using action_probas>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    train_step(states,actions,rewards)\n",
    "            \n",
    "    return sum(rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:38.090\n",
      "mean reward:90.250\n",
      "mean reward:136.730\n",
      "mean reward:87.900\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#That's all, thank you for your attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
